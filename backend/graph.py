
import base64
import uuid
from typing import TypedDict, Annotated, List, Optional, Literal
from sentence_transformers import CrossEncoder
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import operator
from dotenv import load_dotenv
from langchain_cohere import ChatCohere
from langchain_community.tools.tavily_search import TavilySearchResults
from agents.predict_image import predict
from langgraph.checkpoint.memory import InMemorySaver
import os
from agents.vector_store import vector_store
from pydantic import BaseModel, Field
load_dotenv()
reranker_model = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')
def encode_image(image_path: str) -> str:
    """Encode image to base64"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

class QueryAnalysis(BaseModel):
    """Ph√¢n t√≠ch c√¢u h·ªèi ng∆∞·ªùi d√πng: Vi·∫øt l·∫°i c√¢u h·ªèi v√† Ph√¢n lo·∫°i ch·ªß ƒë·ªÅ."""
    condensed_query: str = Field(..., description="C√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i cho r√µ nghƒ©a d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i.")
    query_type: Literal["text_disease", "normal_qa", "chitchat"] = Field(..., description="Lo·∫°i c√¢u h·ªèi: text_disease (b·ªánh c√¢y), normal_qa (h·ªèi ƒë√°p chung), chitchat (x√£ giao).")
class AgricultureState(TypedDict):
    """State definition for the agriculture chatbot"""
    messages: Annotated[List, operator.add]
    user_query: str
    query_type: str
    condensed_query: str
    image_data: Optional[str]
    disease_info: Optional[dict]
    context: dict


# def cosine_similarity(a, b):
#     """T√≠nh cosine similarity gi·ªØa 2 vector numpy."""
#     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
# def condense_conversation_history(state: AgricultureState) -> AgricultureState:
#     """
#     N√©n l·ªãch s·ª≠ h·ªôi tho·∫°i V√Ä x·ª≠ l√Ω th√¥ng tin b·ªï sung.
#     """
#     llm = ChatCohere(model="command-r-plus-08-2024", temperature=0)
#     messages = state.get("messages", [])
#     if not messages:
#         return {"condensed_query": "", "messages": [AIMessage(content="L·ªói: Kh√¥ng c√≥ tin nh·∫Øn.")]}
#
#     user_query = messages[-1].content
#     chat_history = messages[-6:-1]
#     history_str = "\n".join([f"{msg.type}: {msg.content}" for msg in chat_history])
#     if state.get("image_data"):
#         return {
#             **state,
#             "condensed_query": user_query,
#             "user_query": user_query
#         }
#     if not chat_history:
#         return {
#             **state,
#             "condensed_query": user_query,
#             "user_query": user_query
#         }
#
#     try:
#         query_emb = embeddings_model.embed_query(user_query)
#         history_embs = [embeddings_model.embed_query(msg.content) for msg in chat_history]
#         similarities = [cosine_similarity(query_emb, emb) for emb in history_embs]
#
#         max_sim = max(similarities)
#         print(f"[Embedding Similarity] M·ª©c li√™n quan cao nh·∫•t: {max_sim:.3f}")
#
#     except Exception as e:
#         print(f"L·ªói khi t√≠nh embedding similarity: {e}")
#         max_sim = 0.0
#     if max_sim < 0.4:
#         return {
#             **state,
#             "condensed_query": user_query,
#             "user_query": user_query
#         }
#     elif max_sim > 0.6:
#         prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI c√≥ nhi·ªám v·ª• vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
#     D·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i v√† c√¢u h·ªèi m·ªõi, h√£y l√†m theo c√°c quy t·∫Øc sau:
#
#     1.  **Ti·∫øp n·ªëi (Follow-up):** N·∫øu c√¢u h·ªèi m·ªõi l√† c√¢u h·ªèi ti·∫øp n·ªëi (v√≠ d·ª•: "ch·ªØa th·∫ø n√†o?", "nguy√™n nh√¢n?"),
#         h√£y vi·∫øt l·∫°i n√≥ th√†nh m·ªôt c√¢u h·ªèi ƒë·ªôc l·∫≠p, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh t·ª´ l·ªãch s·ª≠.
#         *V√≠ d·ª• L·ªãch s·ª≠: "B·ªánh X"; C√¢u m·ªõi: "C√°ch ch·ªØa?"; K·∫øt qu·∫£: "C√°ch ch·ªØa b·ªánh X?"*
#
#     2.  **B·ªï sung (Correction/Addition):** N·∫øu c√¢u h·ªèi m·ªõi l√† m·ªôt th√¥ng tin **b·ªï sung** ho·∫∑c **s·ª≠a l·ªói** cho c√¢u h·ªèi ngay tr∆∞·ªõc ƒë√≥
#         (v√≠ d·ª•: ng∆∞·ªùi d√πng m√¥ t·∫£ tri·ªáu ch·ª©ng, sau ƒë√≥ n√≥i t√™n c√¢y tr·ªìng),
#         h√£y **k·∫øt h·ª£p** l·ªãch s·ª≠ g·∫ßn nh·∫•t v√† c√¢u m·ªõi th√†nh m·ªôt c√¢u h·ªèi ho√†n ch·ªânh.
#         *V√≠ d·ª• L·ªãch s·ª≠: "...v·∫øt h√¨nh thoi"; C√¢u m·ªõi: "tr√™n c√¢y l√∫a"; K·∫øt qu·∫£: "c√¢y l√∫a c√≥ v·∫øt h√¨nh thoi l√† b·ªánh g√¨?"*
#             L·ªãch s·ª≠: {history_str}
#
#             C√¢u m·ªõi: {user_query}
#             K·∫øt qu·∫£:"""
#         response = llm.invoke(prompt)
#         condensed_query = response.content.strip()
#         return {
#             **state,
#             "condensed_query": condensed_query,
#             "user_query": user_query
#         }
#
#
#     else:
#         prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI c√≥ nhi·ªám v·ª• vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
#     D·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i v√† c√¢u h·ªèi m·ªõi, h√£y l√†m theo c√°c quy t·∫Øc sau:
#
#     1.  **Ti·∫øp n·ªëi (Follow-up):** N·∫øu c√¢u h·ªèi m·ªõi l√† c√¢u h·ªèi ti·∫øp n·ªëi (v√≠ d·ª•: "ch·ªØa th·∫ø n√†o?", "nguy√™n nh√¢n?"),
#         h√£y vi·∫øt l·∫°i n√≥ th√†nh m·ªôt c√¢u h·ªèi ƒë·ªôc l·∫≠p, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh t·ª´ l·ªãch s·ª≠.
#         *V√≠ d·ª• L·ªãch s·ª≠: "B·ªánh X"; C√¢u m·ªõi: "C√°ch ch·ªØa?"; K·∫øt qu·∫£: "C√°ch ch·ªØa b·ªánh X?"*
#
#     2.  **B·ªï sung (Correction/Addition):** N·∫øu c√¢u h·ªèi m·ªõi l√† m·ªôt th√¥ng tin **b·ªï sung** ho·∫∑c **s·ª≠a l·ªói** cho c√¢u h·ªèi ngay tr∆∞·ªõc ƒë√≥
#         (v√≠ d·ª•: ng∆∞·ªùi d√πng m√¥ t·∫£ tri·ªáu ch·ª©ng, sau ƒë√≥ n√≥i t√™n c√¢y tr·ªìng),
#         h√£y **k·∫øt h·ª£p** l·ªãch s·ª≠ g·∫ßn nh·∫•t v√† c√¢u m·ªõi th√†nh m·ªôt c√¢u h·ªèi ho√†n ch·ªânh.
#         *V√≠ d·ª• L·ªãch s·ª≠: "...v·∫øt h√¨nh thoi"; C√¢u m·ªõi: "tr√™n c√¢y l√∫a"; K·∫øt qu·∫£: "c√¢y l√∫a c√≥ v·∫øt h√¨nh thoi l√† b·ªánh g√¨?"*
#     3. **Thay ƒë·ªïi** N·∫øu ng∆∞·ªùi d√πng h·ªèi m·ªôt c√¢u h·ªèi ho√†n to√†n m·ªõi kh√¥ng li√™n quan g√¨ ƒë·∫øn tin nh·∫Øn tr∆∞·ªõc ƒë√≥ h√£y g√¨ nguy√™n c√¢u h·ªèi
#         c·ªßa ng∆∞·ªùi d√πng.V√≠ d·ª• khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ lo·∫°i c√¢y tr·ªìng kh√°c, ho·∫∑c v·∫•n ƒë·ªÉ kh√°c kh√¥ng li√™n quan ƒë·∫øn qu√° kh·ª©.
#
#     L·ªãch s·ª≠:
#     {history_str}
#
#     C√¢u m·ªõi: {user_query}
#
#     K·∫øt qu·∫£ (vi·∫øt l·∫°i ho·∫∑c k·∫øt h·ª£p):"""
#
#         response = llm.invoke(prompt)
#         condensed_query = response.content.strip()
#
#         print(f"[Condenser]: ƒê√£ n√©n th√†nh: {condensed_query}")
#
#         return {
#             **state,
#             "condensed_query": condensed_query,
#             "user_query": user_query
#         }
# def classify_input(state: AgricultureState) -> AgricultureState:
#     """Classify the type of user query"""
#     image_data = state.get("image_data")
#     if image_data:
#         return {
#             **state,
#             "query_type": "image_disease"
#         }
#     llm = ChatCohere(model="command-r-plus-08-2024", temperature=0)
#     classification_prompt = f"""Truy v·∫•n c·ªßa ng∆∞·ªùi d√πng: {state['condensed_query']}
#
# Nhi·ªám v·ª•: Ph√¢n lo·∫°i truy v·∫•n th√†nh **m·ªôt v√† ch·ªâ m·ªôt** trong ba nh√£n sau. H√£y ƒë·ªçc k·ªπ n·ªôi dung ƒë·ªÉ x√°c ƒë·ªãnh ƒë√∫ng ch·ªß ƒë√≠ch.
#
# 1. text_disease
#    - Khi ng∆∞·ªùi d√πng m√¥ t·∫£ **tri·ªáu ch·ª©ng th·ª±c t·∫ø** tr√™n c√¢y, l√°, th√¢n, r·ªÖ, qu·∫£‚Ä¶
#    - Th∆∞·ªùng xu·∫•t hi·ªán c√°c m√¥ t·∫£ nh∆∞: ƒë·ªëm l√°, v√†ng l√°, h√©o r≈©, ch√°y m√©p, n·∫•m, th·ªëi r·ªÖ‚Ä¶
#    - M·ª•c ƒë√≠ch ch√≠nh: **nh·∫≠n di·ªán b·ªánh ho·∫∑c v·∫•n ƒë·ªÅ c·ª• th·ªÉ c·ªßa c√¢y d·ª±a tr√™n tri·ªáu ch·ª©ng.**
#
# 2. normal_qa
#    - Khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ **ki·∫øn th·ª©c n√¥ng nghi·ªáp chung**, kh√¥ng nh·∫±m m√¥ t·∫£ tri·ªáu ch·ª©ng ƒë·ªÉ nh·∫≠n d·∫°ng b·ªánh.
#    - Bao g·ªìm: nguy√™n nh√¢n, c√°ch chƒÉm s√≥c, c√°ch ph√≤ng b·ªánh, quy tr√¨nh tr·ªìng, dinh d∆∞·ª°ng, gi√° n√¥ng s·∫£n, t√°c h·∫°i c·ªßa b·ªánh, thu·ªëc tr·ªã, k·ªπ thu·∫≠t canh t√°c, t∆∞ v·∫•n gi·ªëng‚Ä¶
#    - Kh√¥ng k√®m m√¥ t·∫£ tri·ªáu ch·ª©ng th·ª±c t·∫ø.
#
# 3. chitchat
#    - Khi ng∆∞·ªùi d√πng giao ti·∫øp x√£ giao ho·∫∑c n·ªôi dung **kh√¥ng li√™n quan ƒë·∫øn n√¥ng nghi·ªáp**.
#    - V√≠ d·ª•: ch√†o h·ªèi, c·∫£m ∆°n, khen/ch√™, h·ªèi chuy·ªán c√° nh√¢n, n√≥i linh tinh‚Ä¶
#
#  Ch·ªâ tr·∫£ v·ªÅ **m·ªôt trong ba nh√£n duy nh·∫•t** d∆∞·ªõi ƒë√¢y, kh√¥ng gi·∫£i th√≠ch th√™m:
# - text_disease
# - normal_qa
# - chitchat"""
#     response = llm.invoke([HumanMessage(content=classification_prompt)])
#     query_type = response.content.strip().lower()
#     valid_types = ["text_disease", "normal_qa", "chitchat"]
#     if query_type not in valid_types:
#         query_type = "chitchat"
#     return {
#         **state,
#         "query_type": query_type}
def process_user_query(state: AgricultureState) -> AgricultureState:
    """
 n√©n l·ªãch s·ª≠ v·ª´a ph√¢n lo·∫°i .
    """

    image_data = state.get("image_data")
    if image_data:
        messages = state.get("messages", [])
        user_query = messages[-1].content if messages else ""
        return {
            **state,
            "condensed_query": user_query,
            "user_query": user_query,
            "query_type": "image_disease",
            "disease_info": None
        }

    # 2. Chu·∫©n b·ªã d·ªØ li·ªáu cho LLM
    messages = state.get("messages", [])
    if not messages:
        return {**state, "condensed_query": "", "query_type": "chitchat"}

    user_query = messages[-1].content

    chat_history = messages[-6:-1]
    history_str = "\n".join([f"{msg.type}: {msg.content}" for msg in chat_history])

    llm = ChatCohere(model="command-r-plus-08-2024", temperature=0)

    structured_llm = llm.with_structured_output(QueryAnalysis)

    system_prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia AI v·ªÅ n√¥ng nghi·ªáp. Nhi·ªám v·ª• c·ªßa b·∫°n l√† x·ª≠ l√Ω c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i.

    L·ªäCH S·ª¨ H·ªòI THO·∫†I:
    {history_str}

    C√ÇU H·ªéI M·ªöI: {user_query}
** Nhi·ªám v·ª• 1:
#     D·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i v√† c√¢u h·ªèi m·ªõi, h√£y l√†m theo c√°c quy t·∫Øc sau:
#
#     1.  **Ti·∫øp n·ªëi (Follow-up):** N·∫øu c√¢u h·ªèi m·ªõi l√† c√¢u h·ªèi ti·∫øp n·ªëi (v√≠ d·ª•: "ch·ªØa th·∫ø n√†o?", "nguy√™n nh√¢n?"),
#         h√£y vi·∫øt l·∫°i n√≥ th√†nh m·ªôt c√¢u h·ªèi ƒë·ªôc l·∫≠p, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh t·ª´ l·ªãch s·ª≠.
#         *V√≠ d·ª• L·ªãch s·ª≠: "B·ªánh X"; C√¢u m·ªõi: "C√°ch ch·ªØa?"; K·∫øt qu·∫£: "C√°ch ch·ªØa b·ªánh X?"*
#
#     2.  **B·ªï sung (Correction/Addition):** N·∫øu c√¢u h·ªèi m·ªõi l√† m·ªôt th√¥ng tin **b·ªï sung** ho·∫∑c **s·ª≠a l·ªói** cho c√¢u h·ªèi ngay tr∆∞·ªõc ƒë√≥
#         (v√≠ d·ª•: ng∆∞·ªùi d√πng m√¥ t·∫£ tri·ªáu ch·ª©ng, sau ƒë√≥ n√≥i t√™n c√¢y tr·ªìng),
#         h√£y **k·∫øt h·ª£p** l·ªãch s·ª≠ g·∫ßn nh·∫•t v√† c√¢u m·ªõi th√†nh m·ªôt c√¢u h·ªèi ho√†n ch·ªânh.
#         *V√≠ d·ª• L·ªãch s·ª≠: "...v·∫øt h√¨nh thoi"; C√¢u m·ªõi: "tr√™n c√¢y l√∫a"; K·∫øt qu·∫£: "c√¢y l√∫a c√≥ v·∫øt h√¨nh thoi l√† b·ªánh g√¨?"*
#     3. **Thay ƒë·ªïi** N·∫øu ng∆∞·ªùi d√πng h·ªèi m·ªôt c√¢u h·ªèi ho√†n to√†n m·ªõi kh√¥ng li√™n quan g√¨ ƒë·∫øn tin nh·∫Øn tr∆∞·ªõc ƒë√≥ h√£y g√¨ nguy√™n c√¢u h·ªèi
#         c·ªßa ng∆∞·ªùi d√πng.V√≠ d·ª• khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ lo·∫°i c√¢y tr·ªìng kh√°c, ho·∫∑c v·∫•n ƒë·ªÉ kh√°c kh√¥ng li√™n quan ƒë·∫øn qu√° kh·ª©.
#     K·∫øt qu·∫£ (vi·∫øt l·∫°i ho·∫∑c k·∫øt h·ª£p):
**Nhi·ªám v·ª• 2:
    Ph√¢n lo·∫°i truy v·∫•n th√†nh **m·ªôt v√† ch·ªâ m·ªôt** trong ba nh√£n sau. H√£y ƒë·ªçc k·ªπ n·ªôi dung ƒë·ªÉ x√°c ƒë·ªãnh ƒë√∫ng ch·ªß ƒë√≠ch.
#
# 1. text_disease
#    - Khi ng∆∞·ªùi d√πng m√¥ t·∫£ **tri·ªáu ch·ª©ng th·ª±c t·∫ø** tr√™n c√¢y, l√°, th√¢n, r·ªÖ, qu·∫£‚Ä¶
#    - Th∆∞·ªùng xu·∫•t hi·ªán c√°c m√¥ t·∫£ nh∆∞: ƒë·ªëm l√°, v√†ng l√°, h√©o r≈©, ch√°y m√©p, n·∫•m, th·ªëi r·ªÖ‚Ä¶
#    - M·ª•c ƒë√≠ch ch√≠nh: **nh·∫≠n di·ªán b·ªánh ho·∫∑c v·∫•n ƒë·ªÅ c·ª• th·ªÉ c·ªßa c√¢y d·ª±a tr√™n tri·ªáu ch·ª©ng.**
#
# 2. normal_qa
#    - Khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ **ki·∫øn th·ª©c n√¥ng nghi·ªáp chung**, kh√¥ng nh·∫±m m√¥ t·∫£ tri·ªáu ch·ª©ng ƒë·ªÉ nh·∫≠n d·∫°ng b·ªánh.
#    - Bao g·ªìm: nguy√™n nh√¢n, c√°ch chƒÉm s√≥c, c√°ch ph√≤ng b·ªánh, quy tr√¨nh tr·ªìng, dinh d∆∞·ª°ng, gi√° n√¥ng s·∫£n, t√°c h·∫°i c·ªßa b·ªánh, thu·ªëc tr·ªã, k·ªπ thu·∫≠t canh t√°c, t∆∞ v·∫•n gi·ªëng‚Ä¶
#    - Kh√¥ng k√®m m√¥ t·∫£ tri·ªáu ch·ª©ng th·ª±c t·∫ø.
#
# 3. chitchat
#    - Khi ng∆∞·ªùi d√πng giao ti·∫øp x√£ giao ho·∫∑c n·ªôi dung **kh√¥ng li√™n quan ƒë·∫øn n√¥ng nghi·ªáp**.
#    - V√≠ d·ª•: ch√†o h·ªèi, c·∫£m ∆°n, khen/ch√™, h·ªèi chuy·ªán c√° nh√¢n, n√≥i linh tinh‚Ä¶
#
    """

    try:
        # G·ªçi LLM 1 l·∫ßn duy nh·∫•t
        result = structured_llm.invoke(system_prompt)

        return {
            **state,
            "condensed_query": result.condensed_query,
            "query_type": result.query_type,
            "user_query": user_query
        }

    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω query (fallback v·ªÅ normal_qa): {e}")
        # Fallback an to√†n n·∫øu API l·ªói
        return {
            **state,
            "condensed_query": user_query,
            "query_type": "normal_qa",  # M·∫∑c ƒë·ªãnh coi l√† c√¢u h·ªèi th∆∞·ªùng
            "user_query": user_query
        }
def chitchat(state: AgricultureState) -> AgricultureState:
    """T·∫°o ph·∫£n h·ªìi nhanh cho c√°c c√¢u ch√†o h·ªèi, c·∫£m ∆°n."""
    # B·∫°n c√≥ th·ªÉ d√πng LLM n·∫øu mu·ªën c√¢u tr·∫£ l·ªùi ƒëa d·∫°ng
    llm = ChatCohere(model="command-r-plus-08-2024", temperature=0)

    prompt = f"Ng∆∞·ªùi d√πng: {state['user_query']}. B·∫°n l√† tr·ª£ l√Ω n√¥ng nghi·ªáp th√¢n thi·ªán H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn."
    try:

        response = llm.invoke([SystemMessage("B·∫°n l√† tr·ª£ l√Ω n√¥ng nghi·ªáp th√¢n thi·ªán"),HumanMessage(content=prompt)])

        # X·ª≠ l√Ω k·∫øt qu·∫£ tr·∫£ v·ªÅ
        if isinstance(response, dict):
            # Tr∆∞·ªùng h·ª£p model tr·∫£ v·ªÅ d·∫°ng dict
            if "output" in response:
                content = response["output"]
            elif "messages" in response and response["messages"]:
                content = response["messages"][-1].content
            else:
                content = str(response)
        elif hasattr(response, "content"):
            # Tr∆∞·ªùng h·ª£p l√† AIMessage
            content = response.content
        else:
            # N·∫øu ch·ªâ l√† chu·ªói ho·∫∑c object kh√°c
            content = str(response)

    except Exception as e:
        print(f"[Chitchat Error]: {e}")
        content = "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë. B·∫°n th·ª≠ l·∫°i sau nh√© üå±"

    # Tr·∫£ v·ªÅ ƒë√∫ng d·∫°ng dict m√† LangGraph y√™u c·∫ßu
    return {
        "messages": [AIMessage(content=content)]
    }


def analyze_image(state: AgricultureState) -> AgricultureState:
    """Ph√¢n t√≠ch ·∫£nh"""

    image_data = state.get('image_data')  # L·∫•y d·ªØ li·ªáu base64
    temp_filename = None  # Kh·ªüi t·∫°o ƒë·ªÉ d√πng trong finally

    if not image_data:
        return {
            "disease_info": {"error": "No image provided"},
            "messages": [AIMessage(content="Kh√¥ng c√≥ ·∫£nh n√†o ƒë∆∞·ª£c g·ª≠i l√™n.")]
        }

    try:
        # 1. Decode base64 th√†nh bytes
        image_bytes = base64.b64decode(image_data)

        # 2. T·∫°o th∆∞ m·ª•c t·∫°m n·∫øu ch∆∞a c√≥
        temp_dir = "temp_images"
        if not os.path.exists(temp_dir):
            os.makedirs(temp_dir)

        # 3. T·∫°o t√™n file t·∫°m ng·∫´u nhi√™n
        temp_filename = os.path.join(temp_dir, f"{uuid.uuid4()}.jpg")

        # 4. L∆∞u ·∫£nh v√†o file t·∫°m
        with open(temp_filename, "wb") as f:
            f.write(image_bytes)
        print(f"·∫¢nh t·∫°m ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {temp_filename}")
        response = predict(image_path=temp_filename)


        disease_info = {
            "plant_type": "C√¢y",
            "disease_detected": response.get("label", "Unknown"),
            "confidence": f"{response.get('confidence', 0) * 100:.1f}%"
            }

    finally:
        # X√≥a file t·∫°m sau khi d√πng xong
        if temp_filename and os.path.exists(temp_filename):
            try:
                os.remove(temp_filename)
                print(f"ƒê√£ x√≥a file t·∫°m: {temp_filename}")
            except Exception as delete_error:
                print(f"L·ªói khi x√≥a file t·∫°m {temp_filename}: {delete_error}")

    return {
        "disease_info": disease_info
    }


def request_more_info(state: AgricultureState) -> AgricultureState:
    """T·∫°o m·ªôt tin nh·∫Øn y√™u c·∫ßu ng∆∞·ªùi d√πng g·ª≠i l·∫°i ·∫£nh ho·∫∑c tin nh·∫Øn cung c·∫•p th√™m th√¥ng tin"""
    confidence_str = state['disease_info'].get('confidence', '0%')
    disease_detected = state['disease_info'].get('disease_detected', 'kh√¥ng x√°c ƒë·ªãnh')
    message_content = f"""K·∫øt qu·∫£ ph√¢n t√≠ch ·∫£nh c√≥ ƒë·ªô tin c·∫≠y h∆°i th·∫•p ({confidence_str} cho b·ªánh {disease_detected}).
    ƒê·ªÉ ch·∫©n ƒëo√°n ch√≠nh x√°c h∆°n, b·∫°n vui l√≤ng:
    1.  **G·ª≠i m·ªôt b·ª©c ·∫£nh kh√°c** (r√µ n√©t h∆°n, ƒë·ªß s√°ng, ch·ª•p g·∫ßn khu v·ª±c b·ªã b·ªánh).
    2.  **Ho·∫∑c m√¥ t·∫£ th√™m** v·ªÅ c√°c tri·ªáu ch·ª©ng b·∫°n quan s√°t ƒë∆∞·ª£c"""
    return {
        "messages": [AIMessage(content=message_content)]
    }


def retrieve_knowledge(state: AgricultureState) -> AgricultureState:
    global vector_store, reranker_model
    if not vector_store:
        print("L·ªói: vector_store kh√¥ng ƒë∆∞·ª£c load, b·ªè qua RAG.")
        return {"context": {"retrieved_docs": [], "sources": [], "has_good_content": False}}
    if state.get('disease_info'):
        search_query = f"{state['disease_info'].get('disease_detected', '')} {state['condensed_query']}"
    else:
        search_query = state['condensed_query']

    try:
        initial_docs = vector_store.similarity_search(search_query, k=2)
    except Exception as e:
        print(f"L·ªói Vector Search: {e}")
        initial_docs = []
    final_docs = []
    if initial_docs:
        pairs = [[search_query, doc.page_content] for doc in initial_docs]
        scores = reranker_model.predict(pairs)
        scored_docs = list(zip(initial_docs, scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        RERANK_THRESHOLD = 0.0
        valid_docs = []
        for doc, score in scored_docs:
            print(f"Score: {score:.4f} | Source: {doc.metadata.get('source', 'Unknown')}")
            if score > RERANK_THRESHOLD:
                valid_docs.append(doc)
        if len(valid_docs) > 0:
            final_docs = valid_docs[:1]
        else:
            final_docs = []
    retrieved_contents = [doc.page_content for doc in final_docs]
    sources_list = [doc.metadata.get("source", "Local DB") for doc in final_docs]
    if not final_docs:
        try:
            tavily_tool = TavilySearchResults(max_results=1)
            web_results = tavily_tool.run(search_query)
            if isinstance(web_results, list):
                for res in web_results:
                    content = res.get('content', '')
                    url = res.get('url', 'Web')
                    retrieved_contents.append(f"[Web Search]: {content}")
                    sources_list.append(url)
        except Exception as e:
            print(f"L·ªói Tavily: {e}")
    has_good_context = len(retrieved_contents) > 0
    context = {
        "retrieved_docs": retrieved_contents,
        "sources": sources_list,
        "has_good_context": has_good_context
    }

    print(f"--- Has Good Context: {context['has_good_context']} ---")

    return {
        **state,
        "context": context}


def request_clarification(state: AgricultureState) -> AgricultureState:
    """
    T·∫°o tin nh·∫Øn khi RAG kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan.
    Y√™u c·∫ßu ng∆∞·ªùi d√πng m√¥ t·∫£ l·∫°i.
    """
    message_content = f"""R·∫•t ti·∫øc, t√¥i kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c v·ªÅ "{state['condensed_query']}" trong c∆° s·ªü ki·∫øn th·ª©c c·ªßa m√¨nh.

B·∫°n c√≥ th·ªÉ vui l√≤ng:
1.  **M√¥ t·∫£ l·∫°i c√°c tri·ªáu ch·ª©ng** b·∫±ng t·ª´ ng·ªØ kh√°c?
2.  **Ki·ªÉm tra l·∫°i t√™n** c·ªßa lo·∫°i b·ªánh/c√¢y b·∫°n ƒëang h·ªèi?

ƒêi·ªÅu n√†y s·∫Ω gi√∫p t√¥i t√¨m ki·∫øm ch√≠nh x√°c h∆°n."""

    return {
        "messages": [AIMessage(content=message_content)]
    }


async def generate_disease_diagnosis(state: AgricultureState) -> AgricultureState:
    """Generate detailed disease diagnosis"""
    llm = ChatCohere(model="command-r-plus-08-2024", temperature=0.3)
    context_text = "\n\n".join(state['context'].get('retrieved_docs', []))
    if state['query_type'] == "image_disease":
        disease_context = f"""
        Image Analysis Results:
        - Disease: {state['disease_info'].get('disease_detected', 'Unknown')}
        - Confidence: {state['disease_info'].get('confidence', 'Unknown')}
        """
    else:
        disease_context = f"User's description: {state['condensed_query']}"
    diagnosis_prompt = f"""You are an agricultural consultant. Based on the following information, make a diagnosis of the plant's condition.

    {disease_context}

    Relevant Knowledge:
    {context_text}
    
    Please state clearly:
        1. **Diagnosis:** Disease name and level of confidence.
        2. **Description of symptoms:** (If available in knowledge).
        3.Just extracting words from Relevant Knowledge does not take fabricated information
        4. End with a word of encouragement and an offer of additional support.
        Answer in Vietnamese"""
    try:
        response = await  llm.ainvoke([HumanMessage(content=diagnosis_prompt)])
        final_response_content = response.content.strip()

    # N·∫øu kh√¥ng c√≥ n·ªôi dung (v√≠ d·ª• l·ªói)
        if not final_response_content:
            final_response_content = "Xin l·ªói, t√¥i ch∆∞a th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi l√∫c n√†y."

    except Exception as e:
        print(f"L·ªói invoke format: {e}")  # S·ª≠a t√™n l·ªói
        final_response_content = "L·ªói khi t·∫°o ph·∫£n h·ªìi cu·ªëi c√πng."


    return {
        **state,
        "messages": [AIMessage(content=final_response_content)]
    }


async def generate_normal_qa(state: AgricultureState) -> AgricultureState:
    """Generate response for normal agriculture question"""
    llm = ChatCohere(model="command-r-plus-08-2024", temperature=0.3)
    normal_prompt = f"""You are an expert agricultural advisor. Answer the following question comprehensively.
    Question: {state['condensed_query']}
    Please answer accurately and according to the user's request, do not reply to another topic by mistake. Answer in Vietnamese"""
    try:
        response = await  llm.ainvoke([HumanMessage(content=normal_prompt)])
        final_response_content = response.content.strip()

        # N·∫øu kh√¥ng c√≥ n·ªôi dung
        if not final_response_content:
            final_response_content = "Xin l·ªói, t√¥i ch∆∞a th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi l√∫c n√†y."

    except Exception as e:
        print(f"L·ªói invoke format: {e}")  # S·ª≠a t√™n l·ªói
        final_response_content = "L·ªói khi t·∫°o ph·∫£n h·ªìi cu·ªëi c√πng."

    return {
        **state,
        "messages": [AIMessage(content=final_response_content)]
    }
def create_agriculture_graph():
    memory = InMemorySaver()
    """Create the LangGraph workflow"""
    workflow = StateGraph(AgricultureState)
    # workflow.add_node("condense_history", condense_conversation_history)
    # workflow.add_node("classify", classify_input)
    workflow.add_node("process_user_query",process_user_query)
    workflow.add_node("chitchat", chitchat)
    workflow.add_node("analyze_image", analyze_image)
    workflow.add_node("request_more_info", request_more_info)
    workflow.add_node("retrieve_knowledge", retrieve_knowledge)
    workflow.add_node("request_clarification", request_clarification)
    workflow.add_node("diagnose_disease", generate_disease_diagnosis)
    workflow.add_node("normal_qa", generate_normal_qa)
    # workflow.set_entry_point("condense_history")
    # workflow.add_edge("condense_history", "classify")
    workflow.set_entry_point("process_user_query")

    def route_after_classify(state: AgricultureState) -> str:

        if state['query_type'] == "image_disease":
            return "analyze_image"
        if state['query_type'] == "chitchat":
            return "chitchat"
        else:
            return "retrieve_knowledge"

    workflow.add_conditional_edges(
        "process_user_query",
        route_after_classify,
        {
            "analyze_image": "analyze_image",
            "retrieve_knowledge": "retrieve_knowledge",
            "chitchat": "chitchat"
        }
    )

    def check_confidence(state: AgricultureState) -> str:
        """
        Ki·ªÉm tra ƒë·ªô tin c·∫≠y t·ª´ node analyze_image.
        Ch·ªâ ƒë∆∞·ª£c g·ªçi n·∫øu query_type l√† 'image_disease'.
        """
        try:
            confidence_str = state['disease_info'].get('confidence', '0%')
            disease = state['disease_info'].get('disease_detected', 'Unknown')

            if confidence_str is None:
                # N·∫øu confidence l√† None (do l·ªói ph√¢n t√≠ch), coi nh∆∞ ƒë·ªô tin c·∫≠y th·∫•p
                print("Confidence l√† None. Y√™u c·∫ßu th√™m th√¥ng tin.")
                return "request_more_info"
            # Chuy·ªÉn ƒë·ªïi "number%" th√†nh float
            confidence_val = float(confidence_str.replace('%', '').strip())

            if confidence_val < 70:
                print(f"Ph√°t hi·ªán b·ªánh {disease}.ƒê·ªô tin c·∫≠y th·∫•p ({confidence_val}%), y√™u c·∫ßu th√™m th√¥ng tin.")
                return "request_more_info"
            else:
                print(f"Ph√°t hi·ªán b·ªánh {disease}.ƒê·ªô tin c·∫≠y cao ({confidence_val}%), ti·∫øp t·ª•c truy xu·∫•t.")
                return "retrieve_knowledge"
        except Exception as e:
            # B·∫Øt c√°c l·ªói kh√°c (v√≠ d·ª•: kh√¥ng th·ªÉ chuy·ªÉn 'number%' th√†nh float)
            print(f"L·ªói khi ki·ªÉm tra ƒë·ªô tin c·∫≠y: {e}. Y√™u c·∫ßu th√™m th√¥ng tin.")
            return "request_more_info"

    workflow.add_conditional_edges(
        "analyze_image",
        check_confidence,
        {
            "retrieve_knowledge": "retrieve_knowledge",
            "request_more_info": "request_more_info"
        }
    )

    def route_after_retrieval(state: AgricultureState) -> str:
        if not state['context'].get('has_good_context'):
            print("RAG kh√¥ng t√¨m th·∫•y context t·ªët. Y√™u c·∫ßu l√†m r√µ.")
            return "request_clarification"
        if state['query_type'] in ["image_disease", "text_disease"]:
            return "diagnose_disease"
        else:
            return "normal_qa"

    workflow.add_conditional_edges(
        "retrieve_knowledge",
        route_after_retrieval,
        {
            "diagnose_disease": "diagnose_disease",
            "normal_qa": "normal_qa",
            "request_clarification": "request_clarification"
        }
    )

    workflow.add_edge("diagnose_disease",END)
    workflow.add_edge("normal_qa",END)
    workflow.add_edge("request_more_info", END)
    workflow.add_edge("request_clarification", END)
    workflow.add_edge("chitchat", END)

    return workflow.compile(checkpointer=memory)


app = create_agriculture_graph()
