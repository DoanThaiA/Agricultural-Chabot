# TÃªn file: vector_store_utils.py

import os
import logging
import traceback
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import asyncio
from fastapi.concurrency import run_in_threadpool

# --- 0. Cáº¤U HÃŒNH LOGGING ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

# --- 1. Táº¢I Cáº¤U HÃŒNH Tá»ª .env ---
CHROMA_DB_PATH = os.getenv(
    "CHROMA_DB_PATH",
    r"C:\Laptrinhweb\32_Thai\pythonProject\agents\chroma_db_storage"
)
EMBED_MODEL = os.getenv("EMBED_MODEL", "AITeamVN/Vietnamese_Embedding")

if not CHROMA_DB_PATH or not EMBED_MODEL:
    raise ValueError("CHROMA_DB_PATH hoáº·c EMBED_MODEL chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p trong .env")

# --- 2. KHá»I Táº O EMBEDDING & VECTOR STORE ---
try:
    logger.info(f"ğŸš€ Äang khá»Ÿi táº¡o mÃ´ hÃ¬nh embedding: {EMBED_MODEL}")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}  # giÃºp tÃ­nh cosine chÃ­nh xÃ¡c
    )
    logger.info("âœ… Embedding model Ä‘Ã£ sáºµn sÃ ng.")

    logger.info(f"ğŸ“¦ Äang load/táº¡o vector store táº¡i: {CHROMA_DB_PATH}")
    vector_store = Chroma(
        persist_directory=CHROMA_DB_PATH,
        embedding_function=embeddings,
        collection_metadata={"hnsw:space": "cosine"}
    )
    logger.info(f"âœ… Vector store OK. Tá»•ng sá»‘ vector hiá»‡n cÃ³: {vector_store._collection.count()}")

except Exception as e:
    logger.critical(f"âŒ Lá»–I NGHIÃŠM TRá»ŒNG khi khá»Ÿi táº¡o vector store hoáº·c embedding: {e}")
    traceback.print_exc()
    embeddings = None
    vector_store = None

# --- 3. HÃ€M Xá»¬ LÃ TÃ€I LIá»†U ---

def load_document(temp_file_path: str, original_filename: str):
    """Táº£i tÃ i liá»‡u tá»« file PDF, TXT hoáº·c Ä‘á»‹nh dáº¡ng khÃ¡c."""
    logger.info(f"ğŸ“„ Äang táº£i file: {original_filename}")

    try:
        if original_filename.lower().endswith(".pdf"):
            loader = PyPDFLoader(temp_file_path)
        elif original_filename.lower().endswith(".txt"):
            loader = TextLoader(temp_file_path, encoding="utf-8")
        else:
            loader = UnstructuredFileLoader(temp_file_path)

        docs = loader.load()
        if not docs:
            logger.warning(f"âš ï¸ File {original_filename} khÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c ná»™i dung (cÃ³ thá»ƒ lÃ  PDF scan hoáº·c rá»—ng).")
        else:
            logger.info(f"âœ… ÄÃ£ táº£i {len(docs)} tÃ i liá»‡u tá»« {original_filename}.")
        return docs

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi Ä‘á»c file {original_filename}: {e}")
        traceback.print_exc()
        return []

def split_documents(documents: list):
    """Chia tÃ i liá»‡u thÃ nh cÃ¡c Ä‘oáº¡n nhá» Ä‘á»ƒ embedding."""
    if not documents:
        logger.warning("âš ï¸ KhÃ´ng cÃ³ tÃ i liá»‡u nÃ o Ä‘á»ƒ chia.")
        return []

    logger.info(f"âœ‚ï¸ Äang chia {len(documents)} tÃ i liá»‡u...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=100,
        add_start_index=True
    )

    splits = text_splitter.split_documents(documents)
    logger.info(f"âœ… ÄÃ£ chia thÃ nh {len(splits)} Ä‘oáº¡n vÄƒn báº£n.")
    return splits
def add_documents_to_store(documents: list):
    """ThÃªm cÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Ã£ chia vÃ o ChromaDB."""
    global vector_store
    if not vector_store or not embeddings:
        logger.error("âŒ Vector store hoáº·c Embeddings chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o. Dá»«ng láº¡i.")
        return

    if not documents:
        logger.warning("âš ï¸ KhÃ´ng cÃ³ tÃ i liá»‡u nÃ o Ä‘á»ƒ thÃªm vÃ o vector store.")
        return

    # Lá»c bá» cÃ¡c Ä‘oáº¡n rá»—ng
    non_empty_docs = [doc for doc in documents if doc.page_content.strip()]
    if not non_empty_docs:
        logger.warning("âš ï¸ Táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘á»u trá»‘ng. KhÃ´ng táº¡o embedding.")
        return

    logger.info(f"ğŸ§  Äang thÃªm {len(non_empty_docs)} Ä‘oáº¡n há»£p lá»‡ vÃ o ChromaDB...")
    try:
        vector_store.add_documents(non_empty_docs)
        logger.info(f"âœ… ThÃªm thÃ nh cÃ´ng! Tá»•ng sá»‘ vector hiá»‡n cÃ³: {vector_store._collection.count()}")
    except Exception as e:
        logger.error(f"âŒ Lá»—i khi thÃªm tÃ i liá»‡u vÃ o ChromaDB: {e}")
        traceback.print_exc()

async def process_document_background(temp_path: str, original_name: str):
    """Cháº¡y ná»n Ä‘á»ƒ xá»­ lÃ½ tÃ i liá»‡u Ä‘Æ°á»£c upload."""
    logger.info(f"ğŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ ná»n cho file: {original_name}")
    try:
        docs = await run_in_threadpool(load_document, temp_path, original_name)
        if not docs:
            logger.warning(f"âš ï¸ KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c ná»™i dung tá»« file {original_name}. Bá» qua.")
            return

        splits = await run_in_threadpool(split_documents, docs)

        await run_in_threadpool(add_documents_to_store, splits)
        logger.info(f"ğŸ‰ Xá»­ lÃ½ file {original_name} hoÃ n táº¥t.")
    except Exception as e:
        logger.error(f"âŒ Lá»—i cháº¡y ná»n khi xá»­ lÃ½ {original_name}: {e}")
        traceback.print_exc()
    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)
            logger.info(f"ğŸ§¹ ÄÃ£ xÃ³a file táº¡m: {temp_path}")
